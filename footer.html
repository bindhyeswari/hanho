<!DOCTYPE html>
<html>
<head lang="en">
    <meta charset="UTF-8">
    <title></title>
    <style>
        html, body {
            height: 100%;
            font: 300 1em 'Helvetica Neue';
            margin: 0;
            padding: 0;
        }
        footer {
            text-align: center;
            background-color: #eee;
           /* position: fixed;
            bottom: 0;
            width: 100%;*/
        }
        div.contents {
            margin-bottom: 19px;

        }

        div.wrapper {
            min-height: 100%;
        }
        footer {
            margin-top: -19px;
        }
    </style>
</head>
<body>
    <div class="wrapper">
       <!-- <div class="contents">ikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline use or database queries (such as for Wikipedia:Maintenance). All text content is multi-licensed under the Creative Commons Attribution-ShareAlike 3.0 License (CC-BY-SA) and the GNU Free Documentation License (GFDL). Images and other files are available under different terms, as detailed on their description pages. For our advice about complying with these licenses, see Wikipedia:Copyrights.

            Contents

            1 Where do I get...
            1.1 English-language Wikipedia
            1.2 Other languages
            2 Where are the uploaded files (image, audio, video, etc., files)?
            3 Dealing with compressed files
            4 Dealing with large files
            4.1 File system limits
            4.2 Operating system limits
            4.3 Tips
            5 Why not just retrieve data from wikipedia.org at runtime?
            5.1 Please do not use a web crawler
            5.1.1 Sample blocked crawler email
            5.2 Doing Hadoop MapReduce on the Wikipedia current database dump
            5.3 Doing SQL queries on the current database dump
            6 Database schema
            6.1 SQL schema
            6.2 XML schema
            7 Help parsing dumps for use in scripts
            8 Help importing dumps into MySQL
            9 Static HTML tree dumps for mirroring or CD distribution
            9.1 Kiwix
            9.2 Okawix
            9.3 Aard Dictionary
            9.4 E-book
            9.5 Wikiviewer for Rockbox
            9.6 Old dumps
            10 Dynamic HTML generation from a local XML database dump
            10.1 XOWA
            10.1.1 Features
            10.2 Offline wikipedia reader
            10.2.1 Main features
            10.3 WikiFilter
            10.3.1 WikiFilter system requirements
            10.3.2 How to set up WikiFilter
            10.4 WikiTaxi
            10.4.1 WikiTaxi system requirements
            10.4.2 WikiTaxi usage
            10.5 BzReader and MzReader (for Windows)
            10.6 EPWING
            11 Mirror Building
            11.1 WP-MIRROR
            12 See also
            13 References
            14 External links

            Where do I get...
            English-language Wikipedia

            Dumps from any Wikimedia Foundation project: http://dumps.wikimedia.org/
            English Wikipedia dumps in SQL and XML: http://dumps.wikimedia.org/enwiki/
            pages-articles.xml.bz2 – Current revisions only, no talk or user pages. (This is probably the one you want. The size of the 13 February 2014 dump is approximately 9.85 GB compressed, 44 GB uncompressed).
            As these files are quite large, please consider using a BitTorrent download to reduce the load on our servers. See meta:data dump torrents for more information and links to torrent files.
            Additionally, there are numerous benefits using BitTorrent over a HTTP/FTP download. In an HTTP/FTP download, there is no way to compute the checksum of a file during downloading. Because of that limitation, data corruption is harder to prevent when downloading a large file. With BitTorrent downloads, the checksum of each block/chunk is computed using a SHA-1 hash during the download. If the computed hash doesn't match with the stored hash in the *.torrent file, that particular block/chunk is avoided. As a result, a BitTorrent download is generally more secure against data corruption than a regular HTTP/FTP download.[1]
            pages-meta-current.xml.bz2 – Current revisions only, all pages (including talk)
            abstract.xml.gz – page abstracts
            enwiki-latest-all-titles-in-ns0.gz – Article titles only (with redirects)
            SQL files for the pages and links are also available
            All revisions, all pages: These files expand to multiple terabytes of text. Please only download these if you know you can cope with this quantity of data. Go to Latest Dumps and look out for all the files that have 'pages-meta-history' in their name.
            To download a subset of the database in XML format, such as a specific category or a list of articles see: Special:Export, usage of which is described at Help:Export.
            Wiki front-end software: MediaWiki [1].
            Database backend software: You want to download MySQL.
            Image dumps: See below.

            Other languages

            In the http://dumps.wikimedia.org/ directory you will find the latest SQL and XML dumps for the projects, not just English. For example, (others exist, just select the appropriate language code and the appropriate project):

            Arabic Wikipedia dumps: http://dumps.wikimedia.org/arwiki/
            Dutch Wikipedia dumps: http://dumps.wikimedia.org/nlwiki/
            English Wikipedia dumps: http://dumps.wikimedia.org/enwiki/
            French Wikipedia dumps: http://dumps.wikimedia.org/frwiki/
            German Wikipedia dumps: http://dumps.wikimedia.org/dewiki/

        </div>-->
    </div>


    <footer>&copy;2008-2014 Go Live Labs</footer>



</body>
</html>